#!/usr/bin/env bash

set -e

urls="$("$HOME/bin/wget-spider" "$@" | grep -Piv '\.(woff2|ttf)' || true)"

urls="$(echo "$urls" | perl -pe 's/^http:/https:/g' | sort -u)"

if [[ -n "$urls" ]]
then
	urlCount="$(echo "$urls" | wc -l)"
else
	urlCount=0
fi

echo
echo "$urlCount urls"

logPath="$HOME/.cache/wayback-spider.log"
errorLogPath="$HOME/.cache/wayback-spider-fail.html"
mkdir -p "$HOME/.cache"
saveBaseUrl='https://web.archive.org/save'

if [[ -f "$logPath" ]]
then
	fileModTime=$(stat -c '%Y' "$logPath")
	currentTime=$(date +%s)
	timeDifference=$((currentTime - fileModTime))

	# delete the log if it's older than x hours
	if [[ "$timeDifference" -gt $((60 * 60 * 24)) ]]
	then
		rm "$logPath"
	# otherwise, remove recently checked urls from the list of urls
	# this gives makes recovering failed sessions a little easier
	else
		echo "removing recently checked urls..."
		urls="$(echo "$urls" | grep -Fivx -f "$logPath")"

		if [[ -n "$urls" ]]
		then
			newUrlCount="$(echo "$urls" | wc -l)"
		else
			newUrlCount=0
		fi

		echo "$newUrlCount urls"
		i=$((urlCount-newUrlCount))
	fi
fi

echo '----------'

if [[ -z "$i" ]]
then
	i=0
fi
iExceptions=0

while read -r url
do
	i=$((i+1))

	if [[ -z "$url" ]]
	then
		continue
	fi

	perc="$(echo "scale=4; ($i / $urlCount) * 100" | bc -l | perl -pe 's/0+$//g')%"

	domain="$(echo "$url" | grep -iPo 'https?://[^/]+')"

	formattedUrl="$url"
	formattedUrl="${formattedUrl/#$domain\//}"
	formattedUrl="${formattedUrl/#http*:\/\//}"

	echo -e "\r   ${formattedUrl}\033[K"
	echo -n "$perc"

	saveUrl="$saveBaseUrl/$url"

	while true
	do
		if ! output="$(curl -sL "$saveUrl" | tr -d '\0')"
		then
			echo -e "\rrequest failure\033[K"
			iExceptions=$((iExceptions+1))
		elif [[ "$output" == *"<!-- End Wayback Rewrite JS Include -->"* || "${output,,}" == *"appended by wayback machine"* ]]
		then
			iExceptions=0
			echo -ne "\033[1A\r✓ "
			echo
		elif [[ "${output,,}" == *"you have already reached the limit of active save page now"* || "${output,,}" == *"please wait for a minute and then try again"* ]]
		then
			echo -e "\rrate limit\033[K"
			iExceptions=$((iExceptions+1))
		elif [[ "${output,,}" == *"<p>job failed</p>"* ]]
		then
			echo -e "\r'job failed'\033[K"
			iExceptions=$((iExceptions+1))
		# elif [[ "${url,,}" == *".css" ]]
		# then
		# 	# some file types just return a very plain response
		# 	iExceptions=0
		# 	echo -ne "\033[1A\r✓ "
		# 	echo
		elif [[ "${output,,}" != *wayback* ]]
		then
			iExceptions=0
			echo -e "\033[1A\r? "
			echo -e "\rno wayback mention\033[K"
			# iExceptions=$((iExceptions+1))
		else
			echo
			echo "unexpected output format"
			echo "$saveUrl"
			echo "response saved to $errorLogPath"
			echo "$output" > "$errorLogPath"

			exit 1
		fi

		if [[ "$iExceptions" -ge 10 ]]
		then
			echo "exiting due to $iExceptions consecutive exceptions"
			exit 1
		elif [[ "$iExceptions" -eq 0 ]]
		then
			echo "$url" >> "$logPath"
		fi

		# wayback save limit is 15 urls per minute
		# 60 / 15 = 4s
		# but for some reason that still raises a rate limit error
		# iSeconds="$((15 * (iExceptions + 1)))s"
		iSeconds="$((15 + (iExceptions * 60)))s"
		echo -n "$perc waiting ${iSeconds}"
		sleep "$iSeconds"

		# break out of the attempt loop
		# and move on to the next url
		if [[ "$iExceptions" -eq 0 ]]
		then
			break
		fi
	done
done < <(echo "$urls")
echo
